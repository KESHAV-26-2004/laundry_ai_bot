{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ca149f-7f52-4b25-b2ef-d21d67eb3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9826ca-4173-437b-a969-5ba40d6fed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keshu\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048147ed-78a8-44bf-beee-9d7660585f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2573efa9-ca28-49fa-9db8-46127cdf71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c410fb49-1da0-4b28-9fed-246c638629bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"Passing a tuple of `past_key_values` is deprecated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5dc5d9-327a-4296-a0cb-3130982722b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2443cc28-d5a2-4c6d-b996-6ff873dbd77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"Answer this question: How long does laundry take?\", \n",
    "     \"It typically takes 4 to 5 days to process and complete a laundry order. Check the app for your orderâ€™s estimated completion date.\"),\n",
    "\n",
    "    (\"Answer this question: What is the time taken for laundry?\", \n",
    "     \"Laundry usually takes 4 to 5 days. You can track your order progress in the app's Track Order section.\"),\n",
    "\n",
    "    (\"Answer this question: laundry kitne dino mein milega?\", \n",
    "     \"Aapka laundry order usually 4 se 5 din mein complete ho jata hai. Track Order page se aap status check kar sakte ho.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260bb30-88aa-4084-8cef-12000bd816d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"Datasets//laundry_faq_500_basic.json\", \"r\") as file:\n",
    "#    dataset = json.load(file)\n",
    "\n",
    "# Convert to input-output format for training\n",
    "#qa_pairs = [(\"Answer this question: \" + item[\"question\"], item[\"answer\"]) for item in dataset]\n",
    "\n",
    "\n",
    "# Load pre-trained tokenizer (FLAN-T5 tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Tokenize data\n",
    "def preprocess_data(pairs, tokenizer, max_length=128):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    for question, answer in pairs:\n",
    "        input_enc = tokenizer(question, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        output_enc = tokenizer(answer, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        input_ids = input_enc[\"input_ids\"].squeeze()\n",
    "        output_ids = output_enc[\"input_ids\"].squeeze()\n",
    "        output_ids[output_ids == tokenizer.pad_token_id] = -100  # ignore loss on padding tokens\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        outputs.append(output_ids)\n",
    "\n",
    "    return torch.stack(inputs), torch.stack(outputs)\n",
    "\n",
    "# Prepare tokenized dataset\n",
    "inputs, outputs = preprocess_data(qa_pairs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbdbda6-ee59-41f8-b7f6-f3a8db70cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model intializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c728bd78-8fcb-428a-b79f-a2e0be8283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaundryBot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaundryBot, self).__init__()\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        return self.model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "# Initialize model\n",
    "model = LaundryBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35dce879-1959-4b06-97bc-f3b431766d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b07deda6-4756-447a-8282-6b504910f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: flan_laundry_bot1_8_lr5e-05.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keshu\\AppData\\Local\\Temp\\ipykernel_21152\\44815235.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_filename))\n",
      "C:\\Users\\keshu\\AppData\\Local\\Temp\\ipykernel_21152\\44815235.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Place this before your main training loop\n",
      "Epoch 1/30:   0%|                                                          | 0/1 [00:00<?, ?batch/s]C:\\Users\\keshu\\AppData\\Local\\Temp\\ipykernel_21152\\44815235.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Epoch 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.31s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.44s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.44s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.42s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.49s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.53s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.51s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.46s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.38s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.53s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.46s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.45s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.53s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.46s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.53s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.55s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.51s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.39s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.50s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.44s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.42s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.54s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.44s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.42s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.50s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.49s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.55s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.41s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.51s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.50s/batch, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: nan\n",
      "Model saved as flan_laundry_bot1_8_lr5e-05.pth\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "batch_size = 3\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(inputs, outputs)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load model checkpoint if exists\n",
    "#model_filename = f\"flan_laundry_bot1_{batch_size}_lr{learning_rate}.pth\"\n",
    "model_filename = \"flan_laundry_bot1_8_lr5e-05.pth\"\n",
    "if os.path.exists(model_filename):\n",
    "    print(f\"Loading model from checkpoint: {model_filename}\")\n",
    "    model.load_state_dict(torch.load(model_filename))\n",
    "\n",
    "# Train the model\n",
    "model.to(device)\n",
    "\n",
    "scaler = GradScaler()  # Place this before your main training loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_progress = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\", ncols=100)\n",
    "    \n",
    "    for batch in epoch_progress:\n",
    "        input_ids, labels = batch\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "        epoch_progress.set_postfix(loss=total_loss / (epoch_progress.n + 1))\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Save model after training\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d47d48d-26f3-4335-b9e5-cbe84dc99285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing of bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6ba5bdd-3e49-4b03-a9cd-72f70aaadc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keshu\\AppData\\Local\\Temp\\ipykernel_21152\\959033687.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"flan_laundry_bot1_8_lr5e-05.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Your order will typically take 4 to 5 days to complete.\n"
     ]
    }
   ],
   "source": [
    "# Load trained model for inference\n",
    "model.load_state_dict(torch.load(\"flan_laundry_bot1_8_lr5e-05.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Chatbot response function\n",
    "def get_response(query):\n",
    "    query = \"Answer this question: \" + query\n",
    "    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output_ids = model.model.generate(input_ids, max_length=50)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example interaction\n",
    "user_query = \"How long will my laundry take?\"\n",
    "bot_reply = get_response(user_query)\n",
    "print(f\"Bot: {bot_reply}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edc880fe-7dd2-49c4-9d0f-682dd40da3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Q: How do I place a laundry order?\n",
      "   Bot: Go to the 'Place Order' page, select items you want cleaned, and tap the button at the bottom to confirm your order.\n",
      "\n",
      "2. Q: Where can I place my clothes for laundry?\n",
      "   Bot: You can place your clothes in the laundry room, or you can place them in the drawers at the bottom of the screen.\n",
      "\n",
      "3. Q: Can I place an order anytime?\n",
      "   Bot: Yes, you can place an order anytime from the 'Place Order' screen.\n",
      "\n",
      "4. Q: Is there a limit to how many clothes I can order?\n",
      "   Bot: No, I can order up to 10 clothes per day.\n",
      "\n",
      "5. Q: What types of clothes can I include in my laundry order?\n",
      "   Bot: You can include any type of clothes, such as jeans, for your laundry order.\n",
      "\n",
      "6. Q: How to place an urgent laundry order?\n",
      "   Bot: Go to the 'Place Order' page, select the items you want cleaned, and press the 'Place Order' button.\n",
      "\n",
      "7. Q: How do I place a dry clean order?\n",
      "   Bot: Go to the 'Dry Clean' page, select items for dry cleaning, and press the 'Dry Clean' button.\n",
      "\n",
      "8. Q: What is the price of dry cleaning a blanket?\n",
      "   Bot: The price of dry cleaning a blanket is typically between $50 and $100.\n",
      "\n",
      "9. Q: Which items are allowed in dry clean?\n",
      "   Bot: Items that are not cleaned in the laundry are allowed in the dry clean.\n",
      "\n",
      "10. Q: Can I add dry clean and normal wash in one order?\n",
      "   Bot: Yes, you can add dry clean and normal wash to your order.\n",
      "\n",
      "11. Q: Is dry cleaning available on weekends?\n",
      "   Bot: Yes, on weekends, laundry can be done at the Laundromat.\n",
      "\n",
      "12. Q: How can I track my laundry order?\n",
      "   Bot: You can track your laundry order from the 'Track Order' page in the app.\n",
      "\n",
      "13. Q: What does 'ongoing' status mean?\n",
      "   Bot: The status of an ongoing project is indicated in the 'Ongoing' section.\n",
      "\n",
      "14. Q: How long will my laundry take?\n",
      "   Bot: Your laundry will take approximately 1-2 hours to complete.\n",
      "\n",
      "15. Q: Where can I see order status?\n",
      "   Bot: You can see your current order in the 'Order History' section of the app.\n",
      "\n",
      "16. Q: Can I cancel an ongoing order?\n",
      "   Bot: Yes, you can cancel an existing order at any time by selecting the 'Cancel Order' option at the bottom of the page.\n",
      "\n",
      "17. Q: Where can I see my past orders?\n",
      "   Bot: Your past orders can be seen in the Order History section.\n",
      "\n",
      "18. Q: How to view laundry history?\n",
      "   Bot: You can view your past laundry history in the 'Laundry History' section on the dashboard.\n",
      "\n",
      "19. Q: Can I check previous dry clean orders?\n",
      "   Bot: Yes, in the 'Dry Clean' section, you can see previous dry clean orders.\n",
      "\n",
      "20. Q: Do I have access to all completed orders?\n",
      "   Bot: Yes, you can view and update your completed orders from the 'Order History' page.\n",
      "\n",
      "21. Q: Can I see which items I ordered last time?\n",
      "   Bot: Yes, I ordered the same item last time.\n",
      "\n",
      "22. Q: How to pay for my laundry?\n",
      "   Bot: Open the 'Payments' page from the dashboard, check your pending amount, and select a method like Paytm or UPI to pay.\n",
      "\n",
      "23. Q: What are the payment options?\n",
      "   Bot: Payments can be made via UPI, UPI, or cash on delivery.\n",
      "\n",
      "24. Q: Can I use Paytm for laundry payment?\n",
      "   Bot: Yes, you can pay using UPI or Paytm.\n",
      "\n",
      "25. Q: Where can I see pending payment?\n",
      "   Bot: In the Payments History, you can tap any item to view its status.\n",
      "\n",
      "26. Q: Why is my payment marked pending?\n",
      "   Bot: Your payment is pending, and you can check it from the 'Payments' page.\n",
      "\n",
      "27. Q: How to check my payment history?\n",
      "   Bot: You can view your past payments in the 'Payments' page under Payment History.\n",
      "\n",
      "28. Q: Who made this laundry app?\n",
      "   Bot: This laundry app was developed by Keshav, a BTech student at the University of California, Berkeley.\n",
      "\n",
      "29. Q: Is this app available for staff too?\n",
      "   Bot: Yes, the app is available for both students and laundry staff.\n",
      "\n",
      "30. Q: Can I use this app on iPhone?\n",
      "   Bot: Yes, you can use this app on your iPad or computer.\n",
      "\n",
      "31. Q: Is the app only for Bennett students?\n",
      "   Bot: No, the app is for both Bennett and other students.\n",
      "\n",
      "32. Q: What happens if I forget my password?\n",
      "   Bot: Click on 'Forgot Password' on the login page. You'll receive a reset link or OTP on your Bennett email.\n",
      "\n",
      "33. Q: My order is stuck in processing, what should I do?\n",
      "   Bot: Go to the 'Place Order' section in the app to see if your order is currently in processing.\n",
      "\n",
      "34. Q: I paid but the status is still pending.\n",
      "   Bot: I paid and the status is still pending.\n",
      "\n",
      "35. Q: I selected the wrong clothes, can I change it?\n",
      "   Bot: Yes, you can change the order by selecting the clothes and the quantity.\n",
      "\n",
      "36. Q: My order was not picked up, whom do I contact?\n",
      "   Bot: I'll contact the order management at the POS, and you'll receive a response.\n",
      "\n",
      "37. Q: Can I get a receipt for my payment?\n",
      "   Bot: Yes, you can receive a receipt from the app once you've paid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    # âœ… Laundry Order Placement\n",
    "    \"How do I place a laundry order?\",\n",
    "    \"Where can I place my clothes for laundry?\",\n",
    "    \"Can I place an order anytime?\",\n",
    "    \"Is there a limit to how many clothes I can order?\",\n",
    "    \"What types of clothes can I include in my laundry order?\",\n",
    "    \"How to place an urgent laundry order?\",\n",
    "\n",
    "    # ðŸ§¼ Dry Clean Specific\n",
    "    \"How do I place a dry clean order?\",\n",
    "    \"What is the price of dry cleaning a blanket?\",\n",
    "    \"Which items are allowed in dry clean?\",\n",
    "    \"Can I add dry clean and normal wash in one order?\",\n",
    "    \"Is dry cleaning available on weekends?\",\n",
    "\n",
    "    # ðŸ“¦ Tracking Orders\n",
    "    \"How can I track my laundry order?\",\n",
    "    \"What does 'ongoing' status mean?\",\n",
    "    \"How long will my laundry take?\",\n",
    "    \"Where can I see order status?\",\n",
    "    \"Can I cancel an ongoing order?\",\n",
    "\n",
    "    # ðŸ§¾ Order History\n",
    "    \"Where can I see my past orders?\",\n",
    "    \"How to view laundry history?\",\n",
    "    \"Can I check previous dry clean orders?\",\n",
    "    \"Do I have access to all completed orders?\",\n",
    "    \"Can I see which items I ordered last time?\",\n",
    "\n",
    "    # ðŸ’° Payments\n",
    "    \"How to pay for my laundry?\",\n",
    "    \"What are the payment options?\",\n",
    "    \"Can I use Paytm for laundry payment?\",\n",
    "    \"Where can I see pending payment?\",\n",
    "    \"Why is my payment marked pending?\",\n",
    "    \"How to check my payment history?\",\n",
    "\n",
    "    # ðŸ‘¤ App & User Info\n",
    "    \"Who made this laundry app?\",\n",
    "    \"Is this app available for staff too?\",\n",
    "    \"Can I use this app on iPhone?\",\n",
    "    \"Is the app only for Bennett students?\",\n",
    "    \"What happens if I forget my password?\",\n",
    "\n",
    "    # âš ï¸ Issues & Support\n",
    "    \"My order is stuck in processing, what should I do?\",\n",
    "    \"I paid but the status is still pending.\",\n",
    "    \"I selected the wrong clothes, can I change it?\",\n",
    "    \"My order was not picked up, whom do I contact?\",\n",
    "    \"Can I get a receipt for my payment?\"\n",
    "]\n",
    "\n",
    "\n",
    "for idx, question in enumerate(test_questions, 1):\n",
    "    bot_reply = get_response(question)\n",
    "    print(f\"{idx}. Q: {question}\\n   Bot: {bot_reply}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b722108c-531f-4a68-9a2a-b0074834c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if using gpu and memory(load)\n",
    "############################del model\n",
    "#torch.cuda.empty_cache()  # If using GPU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (CUDA)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
